{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26cc4df",
   "metadata": {},
   "source": [
    "# How to prepare text data for machine learning\n",
    "Machines cannot understand English or any text data by default. The text data needs a special preparation before you can give it to the machine to predict something out of it. In this project, I will practically go through all the required steps such as removing stops words, correcting spelling mistakes, removing meaningless words, removing rare words and many more.\n",
    "</br>\n",
    "</br>\n",
    "In natural language processing’s terms, we will perform three major steps which are feature extraction, text pre-processing and advance text preprocessing. These steps include several sub-steps as well, which we will see down the road.\n",
    "</br>\n",
    "</br>\n",
    "## 1. Feature extraction and text pre-processing\n",
    "</br>\n",
    "The first step of preparing text data is applying feature extraction and basic text pre-processing. In feature extraction and basic text pre-processing there several steps as follows,\n",
    "\n",
    "- Removing Punctuations\n",
    "- Removing HTML tags\n",
    "- Special Characters removal\n",
    "- Removing AlphaNumeric words\n",
    "- Tokenization\n",
    "- Removal of Stopwords\n",
    "- Removing most Frequent words and Rare words\n",
    "- Correcting Spelling mistakes\n",
    "- Lower casing\n",
    "- Stemming\n",
    "- Lemmatization\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccba08",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "In this pproject we shall use amazon’s food review dataset available at [kaggle](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews). Lets now load our dataset using pandas library. . Also, we will drop the duplicate rows present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd809240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r'C:\\Users\\user\\Datasets\\amazonreviews.csv')\n",
    "dataset = dataset.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d50a27",
   "metadata": {},
   "source": [
    "### 1.1 Removing Punctuations\n",
    "The first step is removing punctuations from the text. For humans, it adds value but for the machine, it doesn’t really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e91826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd459794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific\n",
    "#phrase = re.sub(r\"won't\", \"will not\", review)\n",
    "#phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "\n",
    "# general\n",
    "#phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
    "#phrase = re.sub(r\"\\'re\", \" are\", review)\n",
    "#phrase = re.sub(r\"\\'s\", \" is\", review)\n",
    "#phrase = re.sub(r\"\\'d\", \" would\", review)\n",
    "#phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
    "#phrase = re.sub(r\"\\'t\", \" not\", review)\n",
    "#phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
    "#phrase = re.sub(r\"\\'m\", \" am\", review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb776816",
   "metadata": {},
   "source": [
    "### 1.2 Removing HTML tags\n",
    "Okay, now this one is obvious right! When you get the text data from web scrapping and it is very common that you end having HTML tags in your dataset. HTML is for decorating the texts in the Web pages, which is not helpful in the Model building. In general removing HTML tags is good practice to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bcf8ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really like this food\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "review = 'I <strong>really</strong> like this food'\n",
    "soup = BeautifulSoup(review, 'lxml')\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4798af",
   "metadata": {},
   "source": [
    "Output: \n",
    "> I really like this food"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba6cc9",
   "metadata": {},
   "source": [
    "### 1.3 Special Characters removal\n",
    "You might find some words or characters in the dataset which have special characters, which are not helpful in NLP. The best example is the usage of Hashtags in comments. Removing special characters means keeping only alphabets in text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76cd5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really like this food  foodlove  eatfit\n"
     ]
    }
   ],
   "source": [
    "review = 'I really like this food #foodlove #eatfit'\n",
    "review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c849cb3",
   "metadata": {},
   "source": [
    "Output: \n",
    "\n",
    "> I really like this food foodlove eatfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4d94e",
   "metadata": {},
   "source": [
    "### 1.4 Removing AlphaNumeric words\n",
    "Again, AlphaNumeric words don’t help in building a predictive model. These words don’t have meaning, so it’s better to get rid of them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79d2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really like this\n"
     ]
    }
   ],
   "source": [
    "review = 'I really like this food11333'\n",
    "review = re.sub('\\S*\\d\\S*', '', review).strip()\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a83e4",
   "metadata": {},
   "source": [
    "Output: \n",
    "\n",
    "> I really like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a41cce",
   "metadata": {},
   "source": [
    "### 1.5 Tokenization\n",
    "Tokenization means that parsing your text into a list of words. Basically, it helps in other pre-processing steps, such as Removing stop words which is our next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd4573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Food', 'is', 'Very', 'Good']\n"
     ]
    }
   ],
   "source": [
    "review = 'This Food is Very Good'\n",
    "review = review.split()\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35cec2",
   "metadata": {},
   "source": [
    "Output: \n",
    "> ['this', 'food', 'is', 'very', 'good']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb07e29",
   "metadata": {},
   "source": [
    "### 1.6 Removal of Stopwords\n",
    "Stopwords should be removed from the text data, these words are commonly occurring words in text data, for example, is, am, are and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae6aca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This food good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review = 'This food is very good'\n",
    "review = ' '.join(review for review in review.split() if review not in set(stopwords.words('english')))\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca8fbc",
   "metadata": {},
   "source": [
    "Output: \n",
    "\n",
    "> This food good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63600352",
   "metadata": {},
   "source": [
    "### 1.7 Removing most Frequent words and Rare words\n",
    "In the above section, we removed the stops words, which are very common in any text data. Also, there some words which occur in text data very Frequently and most of the time they are so many that their presence becomes useless.\n",
    "\n",
    "In the case of rare words its opposite, their presence is neglected because of Frequent words. When I say rare, that means they are so rare that they are dominated by the noise made by Frequent words and other general words. Below is code to find the most Frequent words and Rare words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f78413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('i', 1), ('love', 2), ('this', 2), (',', 1), ('is', 1), ('so', 1), ('cool', 1), (';', 1), ('you', 1), ('will', 1), ('it', 1)])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "review = 'I love this, this is so cool; you will love it'\n",
    "\n",
    "allWords = nltk.tokenize.word_tokenize(review)\n",
    "allWordDist = nltk.FreqDist(w.lower() for w in allWords) \n",
    "\n",
    "print(allWordDist.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552eb32c",
   "metadata": {},
   "source": [
    "### 1.8 Correcting spelling mistakes\n",
    "There is a situation where you will find a lot of spelling mistakes in text data. It is one of the useful steps in text pre-processing. If you have large dataset it takes time to correct all the wrong words in the dataset Use the below code to correct the spelling mistakes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a953935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this road\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Ti lobe this foad\"\n",
    "\n",
    "print(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b01af9",
   "metadata": {},
   "source": [
    "Output:\n",
    "> I love this road"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69738d6",
   "metadata": {},
   "source": [
    "There is one step that you need to perform before this, sometimes you will find shorthand version of words for example, text is used as txt. So these words must be corrected before applying this step, spelling correction algorithm might convert them into a different word.\n",
    "\n",
    "### 1.9 Converting words into lower case\n",
    "One of the most important steps is converting words into lower case. This will reduce duplicate copies of the same word if they are in different cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dd48a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this food is very good\n"
     ]
    }
   ],
   "source": [
    "review = 'This Food is Very Good'\n",
    "review = review.lower()\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227cd32",
   "metadata": {},
   "source": [
    "Output: \n",
    "> this food is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1b4a4",
   "metadata": {},
   "source": [
    "### 1.10 Stemming\n",
    "Stemming helps in reducing the size of Sparsh matrix. It removes the suffices ‘ed‘, ‘ing‘, ‘y‘ and so on from the words. For example, if your text data has loved and loving gives us a positive hint. So keeping two variations of the same word is not useful.\n",
    "\n",
    "Basically, the idea is to convert the words into its root word, but in this there is one more technique which is more effective than stemming. It’s called Lemmatization, which we will see in the next section. Use the code below for Stemming,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f302cd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love thi food that much, that i took sometim from my hectic schedul and wrote thi review\n"
     ]
    }
   ],
   "source": [
    "# For Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "review = 'I loved this food that much, that I took sometime from my hectic schedule and wrote this review'\n",
    "review = ' '.join([ps.stem(word) for word in review.lower().split()])\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8104bd",
   "metadata": {},
   "source": [
    "Output: \n",
    "> i love thi food that much, that i took sometim from my hectic schedul and wrote thi review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d700aca",
   "metadata": {},
   "source": [
    "### 1.11 Lemmatization\n",
    "Lemmatization is more effective than stemming. It removes the inflectional endings of the word by using the vocabulary and morphological analysis of words.\n",
    "\n",
    "You can use Stemming or Lemmatization depending upon your usage, but I would suggest you to use Lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcc9de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this food that much, that i take sometime from my hectic schedule and write this review\n"
     ]
    }
   ],
   "source": [
    "# For Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "review = 'I loved this food that much, that I took sometime from my hectic schedule and wrote this review'\n",
    "review = ' '.join([lmtzr.lemmatize(word, 'v') for word in review.lower().split()])\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b46b62",
   "metadata": {},
   "source": [
    "Output: \n",
    "> i love this food that much, that i take sometime from my hectic schedule and write this review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de1f47",
   "metadata": {},
   "source": [
    "# 2. Prepare text data for machine learning using Advance text preprocessing\n",
    "So far, we have learned and understood basic text pre-processing, now let’s understand Advance text preprocessing and what we can achieve with it. We will learn how to encode text data into integers and floating point values so that they be applied in machine learning algorithms. In this section, we will learn below-listed topics,\n",
    "\n",
    "- Bag-of-Word Model\n",
    "- Uni-grams, Bi-grams, and N-grams\n",
    "- TF-IDF\n",
    "</br>\n",
    "\n",
    "## 2.1 Bag-of-Words Model\n",
    "The Bag-of-Words model or for short BoW is an advance text pre-processing technique, which is very easy to understand and widely used in document classification. BoW creates a set of words or in other words, it creates a dictionary of words from the single document. Then it converts that dictionary of words into a vector, where each word is a separate dimension. This is achieved by assigning each word a unique number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a5f88",
   "metadata": {},
   "source": [
    "For example, I have two comments on the food product as shown below,\n",
    "\n",
    "C1 = This Food is delicious and I love it\n",
    "\n",
    "C1 = This Food is tasty and I liked it\n",
    "\n",
    "The corresponding Vectors V1 and V2 of the above two comments we will be as follows,\n",
    "\n",
    "For comment This Food is delicious and I loved it (C1), the Vector is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56905f7b",
   "metadata": {},
   "source": [
    "|This |Food |is |delicious|and|I|loved|it|tasty|liked|\n",
    "|:---:|:---:|:--|:------:|:---:|:-|:-----:|:--|:-----:|:-----:|\n",
    "| 1   |  1  | 1 |   1     | 1 |1| 1   | 1|  0  |   0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c071e",
   "metadata": {},
   "source": [
    "For commentThis Food is tasty and I liked it (C2), the Vector is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff0faf",
   "metadata": {},
   "source": [
    "|This |Food |is |delicious|and|I|loved|it|tasty|liked|\n",
    "|:---:|:---:|:--|:------:|:---:|:-|:-----:|:--|:-----:|:-----:|\n",
    "| 1   |  1  | 1 |   0     | 1 |1| 0   | 1|  1  |   1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b95c8",
   "metadata": {},
   "source": [
    "In the above case, the sparsity of the matrix will be low because we have very fewer Zeros. If you have less sparsity in your matrix, then your model will perform well.\n",
    "\n",
    "##### sparsity means Number of zeros available in Matrix.\n",
    "\n",
    "But this would be the general case, so try to keep sparse of the matrix as low as you can. You can use the below code for BoW,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6fe0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Word Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'this food be delicious and i love it',\n",
    "    'this food be tasty and i like it']\n",
    "\n",
    "#set max_features accroding to your size of dataset\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe49dbd",
   "metadata": {},
   "source": [
    "### 2.2 Uni-grams, Bi-grams and N-grams\n",
    "The Uni-grams is the default method used in BoW model while creating Vectors from the text data. Although you can specify which method should be used in the BoW model. The idea here to make sense from Text data, but in some case, uni-grams are not enough.\n",
    "\n",
    "The Uni-grams for review This Food is delicious and I loved it (C1), is shown below,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c5245",
   "metadata": {},
   "source": [
    "|This |Food |is |delicious|and|I|loved|it|\n",
    "|:---:|:---:|:--|:------:|:---:|:-|:-----:|:--|\n",
    "| 1   |  1  | 1 |   1     | 1 |1| 1   | 1| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745cfe7",
   "metadata": {},
   "source": [
    "Remember, In Uni-grams each word is considered as a dimension. Now when you create Bi-grams of this same review it will take each two adjacent words to create a Bi-gram, unlike Uni-grams which takes each word to create gram. The Bi-grams for the above review will look like this,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9a37f",
   "metadata": {},
   "source": [
    "|This Food |Food is |is delicious|....|....|....|....|loved it|\n",
    "|:--------:|:------:|:----------:|:--:|:--:|:--:|:--:|:------:|\n",
    "| 1        |  1     |     1      |  1 | 1  |  1 | 1  |   1    | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45201696",
   "metadata": {},
   "source": [
    "In N-grams, N can be any number and as the number N increases the dimension of the vector is also increases. It adviced that you should not increase number N too much or else you will fail to capture the general case.\n",
    "\n",
    "In the below snippet we are specifying the Tri-grams in BoW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50a12ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Word Model With Tri-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "corpus = [\n",
    "    'this food be delicious and i love it',\n",
    "    'this food be tasty and i like it']\n",
    "\n",
    "#set max_features accroding to your size of dataset\n",
    "cv = CountVectorizer(ngram_range=(1,3), max_features = 1500)\n",
    "X = cv.fit_transform(corpus).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1c2d1",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- TheCountVectorizer() class, we are passing a value to parameter ngram_range as (1,3). The ngram_range parameter expects a tuple.\n",
    "- Now BoW will implement Uni-grams, Bi-Grams, and Tri-grams since we have specified tuple from 1 to 3.\n",
    "- Mind you again this will take a lot of time to execute if you have a big dataset, also it will increase the dimensionality of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd62435",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 TF-IDF ( Term Frequency-Inverse Document Frequency )\n",
    "#### Term Frequency\n",
    "The Term Frequency is the ratio of the particular word occurs in any review, to the length of the review. Hence, the value of Term Frequency for a particular word associated with a single review will be always between 0 to 1. So mathematically you represent Term Frequency as,\n",
    "\n",
    "#### TF = (Number of times word W occurs in the particular row) / (number of words in that row)\n",
    "\n",
    "#### Inverse Document Frequency\n",
    "\n",
    "Inverse Document Frequency is calculated based on the occurrence of any word incomplete data corpus. Its part of information retrieval technique, basically it helps you find the number of occurance of words in a set of documents.\n",
    "\n",
    "#### IDF = log(Number of rows in document corpus) / (number of rows in which word was present)\n",
    "\n",
    "But why to use TF-IDF? Good question, As I said earlier, the TF-IDF is one of information retrival technqiue and used to calculate word frequencies in the perticular document corpus. Withe the help scikit-learn you can find out the word frequencies using TfidfVectorizer class. Use the below code for the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7436def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "corpus = [\n",
    "    'this food be delicious and i love it',\n",
    "    'this food be tasty and i like it' ]\n",
    " \n",
    "\n",
    "# Creating the transform\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "# Get the shape of Vector\n",
    "x = cv.fit_transform(corpus).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c599",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "In this project, we understood how Prepare Text Data for Machine Learning and Deep learning models. With the help of all these techniques, you can improve your models much better. You can also apply parameter tuning and try to observe the difference in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26609683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
