{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f39889",
   "metadata": {},
   "source": [
    "# Tokenization using Spacy Library\n",
    "### What is Tokenization?\n",
    "Tokenization is the task of splitting a text into small segments, called tokens. The tokenization can be at the document level to produce tokens of sentences or sentence tokenization that produces tokens of words or word tokenization that produces tokens of characters.\n",
    "\n",
    "The tokenizer is usually the initial step of the text preprocessing pipeline and works as input for subsequent NLP operations like stemming, lemmatization, text mining, text classification, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5862229",
   "metadata": {},
   "source": [
    "### Spacy Tokenizers\n",
    "In Spacy, the process of tokenizing a text into segments of words and punctuation is done in various steps. It processes the text from left to right.\n",
    "\n",
    "- First, the tokenizer split the text on whitespace similar to the split() function.\n",
    "- Then the tokenizer checks whether the substring matches the tokenizer exception rules. For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "- Next, it checks for a prefix, suffix, or infix in a substring, these include commas, periods, hyphens, or quotes. If it matches, the substring is split into two tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e2d7f",
   "metadata": {},
   "source": [
    "In the example below, we are tokenizing the text using spacy. First, we imported the Spacy library and then loaded the English language model of spacy and then iterate over the tokens of doc objects to print them in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "best\n",
      "programming\n",
      "languages\n",
      "for\n",
      "beginners\n",
      ".\n",
      "Its\n",
      "syntax\n",
      "is\n",
      "similar\n",
      "to\n",
      "English\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Python is one of the best programming languages for beginners. Its syntax is similar to English.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768108e",
   "metadata": {},
   "source": [
    "### Creating Tokenizer\n",
    "In spacy we can add our own created tokenizer in the pipeline very easily. For example in the code below we are adding the blank Tokenizer with just the English vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d7280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank tokenizer : Let's, go, to, N.Y., "
     ]
    }
   ],
   "source": [
    "# Construction 1\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"Blank tokenizer\",end=\" : \")\n",
    "for token in tokens:\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b18de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default tokenizer : Let, 's, go, to, N.Y., "
     ]
    }
   ],
   "source": [
    "# Construction 2\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a Tokenizer with the default settings for English\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"\\nDefault tokenizer\",end=' : ')\n",
    "for token in tokens:\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fb4d6",
   "metadata": {},
   "source": [
    "### Adding Special Rule\n",
    "\n",
    "In some cases, we need to customize our tokenization rules. This could be an expression or an abbreviation for example if we have WycliffeMwebi in our text and we need to split it (Wycliffe Mwebi) during tokenization. This could be done by adding a special rule to an existing Tokenizer rule., ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a2ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal tokenization :  WycliffeMwebi, is, a, respected, authority, in, Natural, Language, Processing, \n",
      "Special case tokenization :  Wycliffe, Mwebi, is, an, authority, in, Natural, Language, processing, "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"WycliffeMwebi is a respected authority in Natural Language Processing\")  # phrase to tokenize\n",
    "print(\"Normal tokenization : \",end=' ')\n",
    "for token in doc:\n",
    "    print(token,end=', ')\n",
    "\n",
    "special_case = [{ORTH: \"Wycliffe\"}, {ORTH: \"Mwebi\"}]        # Adding special case rule\n",
    "nlp.tokenizer.add_special_case(\"WycliffeMwebi\", special_case)\n",
    "doc = nlp(\"WycliffeMwebi is an authority in Natural Language processing\")\n",
    "\n",
    "print(\"\\nSpecial case tokenization : \",end=' ')\n",
    "for token in doc:      # Checking new tokenization\n",
    "    print(token,end=', ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86b742",
   "metadata": {},
   "source": [
    "### Debugging the Tokenizer\n",
    "A debugging tool was provided by spacy library as nlp.tokenizer.explain(text) which return returns a list of tuples containing token itself and the rules on which it was tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff40ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "move \t TOKEN\n",
      "to \t TOKEN\n",
      "L.A. \t TOKEN\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = \"Let's move to L.A.\"\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b37cd8",
   "metadata": {},
   "source": [
    "### Customizing Spacy Tokenizer\n",
    "In Spacy, we can create our own tokenizer with our own customized rules. For example, if we want to create a tokenizer for a new language, this can be done by defining a new tokenizer method and adding rules of tokenizing to that method. These rules are prefix searches, infix searches, postfix searches, URL searches, and defining special cases. The below code is an example of creating a tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c9cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9350b02",
   "metadata": {},
   "source": [
    "### Modifying Existing Rules of Tokenizer\n",
    "In spacy, we can also modify existing rules of tokenizer by adding or removing the character from the prefix, suffix, or infix rules with the help of NLP’s Default object.\n",
    "\n",
    "#### 1. Adding characters in the suffixes search\n",
    "In the code below we are adding ‘+’, ‘-‘ and ‘$’ to the suffix search rule so that whenever these characters are encountered in the suffix, could be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c618627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tokenized text : This, is+, a-, tokenizing$, sentence, ., \n",
      "Text after adding suffixes : This, is, +, a, -, tokenizing, $, sentence, ., "
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "\n",
    "nlp = English()\n",
    "text = \"This is+ a- tokenizing$ sentence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Default tokenized text\",end=' : ')\n",
    "for token in doc:\n",
    "    print(token,end=', ')\n",
    "\n",
    "suffixes = nlp.Defaults.suffixes + [r\"\\-|\\+|\\$\",]\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "\n",
    "print('\\nText after adding suffixes', end=' : ')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token,end=', ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d3e03",
   "metadata": {},
   "source": [
    "#### 2. Removing characters from the suffix search\n",
    "As we have added suffixes in our suffix rule, similarly we can remove some suffixes from our suffix rule. For example, we are removing the suffix tokenization ‘(‘ from the suffix rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647e731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tokenized text : This, is, a, (, tokenizing, ), sentence, ., \n",
      "Text after removing suffixes : This, is, a, (, tokenizing, ), sentence, ., "
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "\n",
    "nlp = English()\n",
    "text = \"This is a (tokenizing) sentence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Default tokenized text\",end=' : ')\n",
    "for token in doc:\n",
    "    print(token,end=', ')\n",
    "\n",
    "suffixes = list(nlp.Defaults.suffixes)\n",
    "suffixes.remove(\"\\(\")\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "\n",
    "print('\\nText after removing suffixes', end=' : ')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906ede9",
   "metadata": {},
   "source": [
    "### Adding a Custom Tokenizing Class\n",
    "We can customize the tokenizing class of an nlp doc’s object. This can be done by creating a tokenizer class and assigning it to nlp.tokenize.For example, we can add a basic white space tokenizer to our tokenizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06685a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'purpose', 'of', 'our', 'lives', 'is', 'to', 'be', 'happy.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "doc = nlp(\"The purpose of our lives is to be happy.\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a5a15",
   "metadata": {},
   "source": [
    "### Third-party Tokenizers (BERT word pieces)\n",
    "We can also add any other third-party tokenizers. In the example below, we used BERT word piece tokenizer provided by the tokenizers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beab5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]donald john trump is an american politician, media personality, and businessman who served as the 45th president of the united states.[SEP]  \n",
      " ['[CLS]', 'donald', 'john', 'trump', 'is', 'an', 'american', 'politician', ',', 'media', 'personality', ',', 'and', 'businessman', 'who', 'served', 'as', 'the', '45th', 'president', 'of', 'the', 'united', 'states', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "class BertTokenizer:\n",
    "    def __init__(self, vocab, vocab_file, lowercase=True):\n",
    "        self.vocab = vocab\n",
    "        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self._tokenizer.encode(text)\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n",
    "            words.append(text)\n",
    "            if i < len(tokens.tokens) - 1:\n",
    "                # If next start != current end we assume a space in between\n",
    "                next_start, next_end = tokens.offsets[i + 1]\n",
    "                spaces.append(next_start > end)\n",
    "            else:\n",
    "                spaces.append(True)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = BertTokenizer(nlp.vocab, r\"C:\\Users\\user\\NLP\\bert-base-uncased-vocab.txt\")\n",
    "doc = nlp(\"Donald John Trump is an American politician, media personality, and businessman who served as the 45th president of the United States.\")\n",
    "print(doc.text, '\\n', [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbed03",
   "metadata": {},
   "source": [
    "### Merging and Splitting of Tokens\n",
    "We can merge or split our tokens during the process of tokenization by using Doc.retokenize context manager. Modifications in the tokenization are stored and performed all at once when the context manager exits.\n",
    "\n",
    "To merge several tokens into one single token, pass a Span to tokenizer.merge.\n",
    "\n",
    "#### 1. Merging Tokens\n",
    "We can merge several tokens to a single token by passing a span of doc to tokenizer.merge. An optional dictionary of attrs lets us set attributes that will be assigned to the merged token – for example, the lemma, part-of-speech tag, or entity type. By default, the merged token will receive the same attributes as the merged span’s root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "effade28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['I', 'live', 'in', 'New', 'York']\n",
      "After: ['I', 'live', 'in', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I live in New York\")\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "print(\"After:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5c7dc",
   "metadata": {},
   "source": [
    "#### 4. Splitting Tokens\n",
    "We can split a token into two or more tokens using retokenizer.split method. For example, if we want to split ‘NarendraModi’ into two ‘Narendra’ and ‘Modi’, we can apply this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eb01b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['Lord', 'JesusChrist', 'is', 'our', 'Saviour']\n",
      "After: ['Lord', 'Jesus', 'Christ', 'is', 'our', 'Saviour']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Lord JesusChrist is our Saviour\")\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    heads = [(doc[1], 0), (doc[1], 1)]\n",
    "    retokenizer.split(doc[1], [\"Jesus\", \"Christ\"], heads=heads)\n",
    "print(\"After:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f08067",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "Sentence tokenization is the process of splitting sentences of a text. We can iterate over the sentences by using Doc.sents. We can check whether a Doc has sentence boundaries by calling Doc.has_annotation with the attribute name “SENT_START”.\n",
    "\n",
    "The below code is an example of splitting a text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "933b895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "print(doc.has_annotation(\"SENT_START\"))\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673eee1",
   "metadata": {},
   "source": [
    "However, SpaCy has provided four methods to tokenize a sentence. These are :\n",
    "\n",
    "- Dependency parser method\n",
    "- Statistical sentence segmenter method\n",
    "- Rule-based pipeline component method\n",
    "- Custom function method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660cbc5",
   "metadata": {},
   "source": [
    "#### 1. Default: Using the Dependency Parse\n",
    "The spaCy library uses the full dependency parse to determine sentence boundaries. This is usually the most accurate approach and is the default sentence segmenter, but it requires a trained pipeline that provides accurate predictions. This method works with excellent accuracy if our text is closer to general-purpose news or web text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12b1bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ba0ac",
   "metadata": {},
   "source": [
    "#### 2. Statistical Sentence Segmenter\n",
    "The statistical SentenceRecognizer is a simpler and faster alternative to the parser that only provides sentence boundaries. It is easier to train because it only requires annotated sentence boundaries rather than full dependency parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a50e461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f725a",
   "metadata": {},
   "source": [
    "#### 3. Rule-based Pipeline Component\n",
    "The rule-based Sentencizer sets sentence boundaries using a customizable list of sentence-final punctuation like ., ! or ?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caf7c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()  # just the language with no pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f27192",
   "metadata": {},
   "source": [
    "#### 4. Custom Rule-Based Strategy\n",
    "If we want to implement our own strategy that differs from the default rule-based approach of splitting sentences, we can also create a custom pipeline component that takes a Doc object and sets the Token.is_sent_start attribute on each individual token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2694d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['this is a sentence...hello...and another sentence.']\n",
      "After: ['this is a sentence...', 'hello...', 'and another sentence.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "import spacy\n",
    "\n",
    "text = \"this is a sentence...hello...and another sentence.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Before:\", [sent.text for sent in doc.sents])\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "doc = nlp(text)\n",
    "print(\"After:\", [sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec4bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
