{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6a035b",
   "metadata": {},
   "source": [
    "# NLP -Sentiment Analysis\n",
    "Sentiment Analysis is also known as Opinion mining. With the help of Sentiment Analysis, we humans can determine whether the text is showing positive or negative sentiment. Sentiment analysis can help in reducing churn, increase sales of a product, create brand awareness and to analyze the reviews of customers inorder to improve your products.\n",
    "\n",
    "In this project, I will create an NLP machine learning model to predict if a new incoming customer review is positive or negative. I will use amazon’s food review dataset available at [kaggle](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866fff4",
   "metadata": {},
   "source": [
    "## Feature extraction and Text pre-processing\n",
    "Machines can not understand English or any text data by default. The text data needs a special preparation before you can give text data to the machine to predict something out of it. That special preparation includes several steps such as removing stops words, correcting spelling mistakes, removing meaningless words, removing rare words and many more.\n",
    "\n",
    "The first step of preparing text data is applying feature extraction and basic text pre-processing. In feature extraction and basic text pre-processing there several steps as follows,\n",
    "\n",
    "- Removing Punctuations\n",
    "- Removing HTML tags\n",
    "- Special Characters removal\n",
    "- Removing AlphaNumeric words\n",
    "- Tokenization\n",
    "- Removal of Stopwords\n",
    "- Lower casing\n",
    "- Lemmatization\n",
    "\n",
    "#### Load our dataset\n",
    "\n",
    "Before we apply feature extraction and text pre-processing, lets first load out dataset using pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea42a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NLP sentiment analysis in python\n",
    "\"\"\"\n",
    " \n",
    "import pandas as pd\n",
    " \n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r'C:\\Users\\user\\Datasets\\amazonreviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604280b1",
   "metadata": {},
   "source": [
    "Lets view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c64cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d40c21",
   "metadata": {},
   "source": [
    "Let’s import libraries for text pre-processing and later we will use these libraries to do the basic text pre-processing.\n",
    "\n",
    "=> We will import bs4 for Removing HTML tags from the text.\n",
    "\n",
    "=> The re library will help in Removing Alphanumeric Text and Special Characters.\n",
    "\n",
    "=> And As always nltk library is useful in so many ways and we will use it later in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed772ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca2ca6",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "Next will start with removing punctuations from the text. For humans, it adds value but for the machine, it isn’t really useful.\n",
    "\n",
    "=> The re library will be helpful to remove Punctuations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abb05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeApostrophe(review):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", review)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ee942",
   "metadata": {},
   "source": [
    "### Removing HTML tags\n",
    "When you get the text data from web scrapping, it is very common that you end having HTML tags in your dataset. HTML is for decorating the texts in the Web pages, which is not helpful in Model building.\n",
    "\n",
    "=> Here we will use The bs4 library to remove HTML tags.\n",
    "\n",
    "=> In general, removing HTML tags good practice to follow,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516e2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTMLTags(review):\n",
    "    soup = BeautifulSoup(review, 'lxml')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5d729",
   "metadata": {},
   "source": [
    "### Special Characters removal\n",
    "You might find some words or characters in the dataset which have special characters, which are not helpful in NLP. The best example is the usage of Hashtags in comments.\n",
    "\n",
    "=> To remove Special Characters we will use the re library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5ac05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialChars(review):\n",
    "    return re.sub('[^a-zA-Z]', ' ', review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f860a3",
   "metadata": {},
   "source": [
    "### Removing AlphaNumeric words\n",
    "Again, AlphaNumeric words don’t help in building a predictive model. These words don’t have meaning, so it’s better to get rid of them as well.\n",
    "\n",
    "=> To remove Special Characters we will use the re library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc6cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAlphaNumericWords(review):\n",
    "    return re.sub(\"\\S*\\d\\S*\", \"\", review).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b39b96",
   "metadata": {},
   "source": [
    "### Tokenization, Removing Stopwords, Lowercasing, and Lemmatization\n",
    "In this section we will perform Tokenization, Removing Stopwords, Lowercasing, and Lemmatization. Tokenization means that parsing your text into a list of words. Basically, it helps in other pre-processing steps, such as Removing stop words which is our next point. stopwords should be removed from the text data, these words are commonly occurring words in text data, for example, is, am, are and so on. One of the most important steps is converting words into lower case. This will reduce duplicate copies of the same word if they are in different cases. Lemmatization removes the inflectional endings of the word by using the vocabulary and morphological analysis of words.\n",
    "\n",
    "=> We will create a doTextCleaning() function, which will use the above-created methods.\n",
    "\n",
    "=> Also, in this method we will perform Tokenization, Removing Stopwords, Lowercasing, and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab4b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTextCleaning(review):\n",
    "    review = removeHTMLTags(review)\n",
    "    review = removeApostrophe(review)\n",
    "    review = removeAlphaNumericWords(review)\n",
    "    review = removeSpecialChars(review) \n",
    "\n",
    "    review = review.lower()  # Lower casing\n",
    "    review = review.split()  # Tokenization\n",
    "    \n",
    "    # Removing Stopwords and Lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    review = [lmtzr.lemmatize(word, 'v') for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    review = \" \".join(review)    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd407bd",
   "metadata": {},
   "source": [
    "### Creating Document Corpus and Advance text preprocessing\n",
    "In this section will use make use of all the functions that we have created till now and we will perform Advance text preprocessing on the reviews.\n",
    "\n",
    "Now we will create document corpus on which we will apply Bag of words model. The document corpus is a collection of all reviews in the document, where the document is your dataset.\n",
    "\n",
    "=> In the below code we created corpus array and we have applied for loop on our dataset.\n",
    "\n",
    "=> In the for loop we will calldoTextCleaning() function, which will return the cleaned text review. Once we receive the cleaned and preprocessed text, we will append it into the corpus array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166f3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc2f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:02, 61.69it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "333401it [1:48:04, 48.31it/s] "
     ]
    }
   ],
   "source": [
    "corpus = []   \n",
    "for index, row in tqdm(dataset.iterrows()):\n",
    "    review = doTextCleaning(row['Text'])\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b8e49",
   "metadata": {},
   "source": [
    "The next step is to perform Advance text preprocessing on the reviews, which will convert the reviews into Numeric Vectors and using that we can create our Machine Learning model.\n",
    "\n",
    "Using the document corpus we create Bag of Word model along with applying Tri-grams. Bag of Word model creates a set of words or in other words, it creates a dictionary of words from the single document. Then it converts that dictionary of words into a vector, where each word is a separate dimension.\n",
    "\n",
    "Grams(Tri-gram) are useful in creating the word dimensions from the document corpus. The Uni-grams is the default method used in BoW model while creating Vectors from the text data. Although you can specify which method should be used in the BoW model. But here we will use Tri-grams to create word dimension.\n",
    "\n",
    "=> First thing first, import theCountVectorizer transform from scikit library.\n",
    "\n",
    "=> We are creating the transform usingCountVectorizer with tri-grams.\n",
    "\n",
    "=> You can specify which grams you want to use ngram_range parameter, for tri-gram usengram_range=(1,3).\n",
    "\n",
    "=> At the end, we have two vectors to create Machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creating the transform with Tri-gram\n",
    "cv = CountVectorizer(ngram_range=(1,3), max_features = 2)\n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6716fdc",
   "metadata": {},
   "source": [
    "### Building NLP sentiment analysis Machine learning model\n",
    "Now the last the part of the NLP sentiment analysis is to create Machine learning model. In this project, I will use the Naive Bayes classification model.\n",
    "\n",
    "As of now, we have two vectors i.e. X and Y. The first step to create a machine learning model is to split the dataset into the Training set and Test set. Using the training set I will create a Naive Bayes classification model. Then With the test set I can check the performance of the Naive Bayes classification model.\n",
    "\n",
    "=> In the below code, first I have imported the train_test_split API to split the vectors into test and traing set.\n",
    "\n",
    "=> I have importedGaussianNB() class to create a Naive Bayes classification model.\n",
    "\n",
    "=> After creating the Naive Bayes classification model, then I will fit the training set into the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Creating Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Fitting the training set into the Naive Bayes classifier\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2763b",
   "metadata": {},
   "source": [
    "NLP sentiment analysis In Action\n",
    "Now that our model is ready to predict the sentiments based on the Reviews, so why not write a code to test it? By doing this we will understand how well our model is predicting the result and that our end goal as well. So the steps are very straight forward here,\n",
    "\n",
    "=> First we have createdpredictNewReview() function, which will ask to write a review in CMD and then it will use the above-created classifier to predict the sentiment.\n",
    "\n",
    "=> As soon aspredictNewReview() function will get a new review it will do all the text cleaning process usingdoTextCleaning() function.\n",
    "\n",
    "=> Once the text cleaning is performed, then using BOW model transform we will convert the Review the numeric vector.\n",
    "\n",
    "=> After the conversion, the Naive Bayes classification model can be used to predict the result using classifier.predict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict sentiment for new Review\n",
    "def predictNewReview():\n",
    "    newReview = input(\"Type the Review: \")\n",
    "    \n",
    "    if newReview =='':\n",
    "        print('Invalid Review')  \n",
    "    else:\n",
    "        newReview = doTextCleaning(newReview)\n",
    "        reviewVector = cv.transform([newReview]).toarray()  \n",
    "        prediction =  classifier.predict(reviewVector)\n",
    "        if prediction[0] == 1:\n",
    "            print( \"Positive Review\" )\n",
    "        else:        \n",
    "            print( \"Negative Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88438e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
