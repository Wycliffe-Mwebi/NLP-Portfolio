{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9284abc1",
   "metadata": {},
   "source": [
    "# Tokenization using NLTK Library in Python\n",
    "### What is Tokenization in NLP?\n",
    "Tokenization is the process of breaking up the original raw text into component pieces which are known as tokens. Tokenization is usually the initial step for further NLP operations like stemming, lemmatization, text mining, text classification, sentiment analysis, language translation, chatbot creation, etc.\n",
    "\n",
    "Almost always while working with NLP projects you will have to tokenize the data.\n",
    "\n",
    "\n",
    "### What are Tokens?\n",
    "Tokens are broken pieces of the original text that are produced after tokenization. Tokens are the basic building blocks of text -everything that helps us understand the meaning of the text is derived from tokens and the relationship to one another. For example, the character is a token in a word, a word is a token in a sentence, and a sentence is a token in a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f2146",
   "metadata": {},
   "source": [
    "### NLTK Tokenize Package\n",
    "nltk.tokenize is the package provided by the NLTK module that is used in the process of tokenization.\n",
    "\n",
    "In order to install the NLTK package run the following command.\n",
    "\n",
    "- pip install nltk\n",
    "- Then, enter the Python shell in your terminal by simply typing python\n",
    "- Type import nltk\n",
    "- nltk.download(‘all’)\n",
    "\n",
    "### 1. Character Tokenization in Python\n",
    "Character tokenization is the process of breaking text into a list of characters. This can be achieved quite easily in Python without the need for the NLTK library.\n",
    "\n",
    "Let us understand this with the help of an example. In the example below, we used list comprehension to convert the text into a list of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc30a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "text=\"Hello world\"\n",
    "lst=[x for x in text]\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85090a81",
   "metadata": {},
   "source": [
    "### 2. Word Tokenization with NLTK word_tokenize()\n",
    "Word tokenization is the process of breaking a string into a list of words also known as tokens. In NLTK we have a module word_tokeinize() to perform word tokenization.\n",
    "\n",
    "Let us understand this module with the help of an example.\n",
    "\n",
    "In the examples below, we have passed the string sentence to word_tokenize() and tokenize it into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7722388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'Welcome', 'to', 'the', 'programming', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hello there! Welcome to the programming world.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd697835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"We are learning Natural Language Processing.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5336cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'very', 'beautiful', 'girlfrind', 'is', 'pregnant']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 3\n",
    "nltk.word_tokenize('My very beautiful girlfrind is pregnant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8258a5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'own', 'world', '.', 'Is', 'it', 'real']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 4\n",
    "nltk.word_tokenize(\"my own world. Is it real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd594cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last',\n",
       " 'week',\n",
       " ',',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Cambridge',\n",
       " 'shared',\n",
       " 'its',\n",
       " 'own',\n",
       " 'research',\n",
       " 'that',\n",
       " 'shows',\n",
       " 'if',\n",
       " 'everyone',\n",
       " 'wears',\n",
       " 'a',\n",
       " 'mask',\n",
       " 'outside',\n",
       " 'home',\n",
       " ',',\n",
       " 'dreaded',\n",
       " '‘',\n",
       " 'second',\n",
       " 'wave',\n",
       " '’',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pandemic',\n",
       " 'can',\n",
       " 'be',\n",
       " 'avoided',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 5\n",
    "nltk.word_tokenize(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da6e58",
   "metadata": {},
   "source": [
    "### 3. Sentence Tokenization with NLTK sent_tokenize()\n",
    "Sentence tokenization is the process of breaking a paragraph or a string containing sentences into a list of sentences. In NLTK, sentence tokenization can be done using sent_tokenize().\n",
    "\n",
    "In the examples below, we have passed text of multiple lines to sent_tokenize() which tokenizes it into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbf268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It’s easy to point out someone else’s mistake.', 'Harder to recognize your own.']\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"It’s easy to point out someone else’s mistake. Harder to recognize your own.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5330a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Laughing at our mistakes can lengthen our own life.', \"Laughing at someone else's can shorten it.\"]\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Laughing at our mistakes can lengthen our own life. Laughing at someone else's can shorten it.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bfcca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The year was 1991.',\n",
       " 'It was a dangerous time.',\n",
       " 'The Country was agitating for political puluralism.',\n",
       " 'Many leaders were arrested and changed with sedition.',\n",
       " 'Among them was Raila Odinga - A son of former vice president']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 3\n",
    "nltk.sent_tokenize('The year was 1991. It was a dangerous time. The Country was agitating for political puluralism. Many leaders were arrested and changed with sedition. Among them was Raila Odinga - A son of former vice president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e7cbc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 4\n",
    "nltk.sent_tokenize(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaaa61d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives.',\n",
       " 'Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others.',\n",
       " 'Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels?',\n",
       " 'Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves.',\n",
       " 'This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g.',\n",
       " 'radio, movies, television, the internet, mobiles) and the zeitgeist (e.g.',\n",
       " 'cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens.',\n",
       " 'Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 5\n",
    "nltk.sent_tokenize(\"\"\"The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc67a3b",
   "metadata": {},
   "source": [
    "### 4. Whitespace Tokenization with NLTK WhitespaceTokenizer()\n",
    "WhitespaceTokenizer() module of NLTK tokenizes a string on whitespace (space, tab, newline). It is an alternate option for split().\n",
    "\n",
    "In the example below, we have passed a sentence to WhitespaceTokenizer() which then tokenizes it based on the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "950b7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "s=\"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "Tokenizer=WhitespaceTokenizer()\n",
    "print(Tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b6a62",
   "metadata": {},
   "source": [
    "### 5. Word Punctuation Tokenization with NLTK WordPunctTokenizer()\n",
    "WordPunctTokenizer() module of NLTK tokenizes a string on punctuations.\n",
    "In the below example, we have tokenized the string on punctuations by passing it to WordPuntTokenizer() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b776ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', \"'\", 're', 'moving', 'to', 'L', '.', 'A', '.!']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text=\"We're moving to L.A.!\"\n",
    "Tokenizer=WordPunctTokenizer()\n",
    "print(Tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cecfa5",
   "metadata": {},
   "source": [
    "### 6. Removing Punctuations with NLTK RegexpTokenizer()\n",
    "We can remove punctuation from a sentence by using RegexpTokenizer() function of NLTK.\n",
    "In the example below, we have used RegexpTokenizer() to convert the text into a list of words that don’t contain punctuations and then joined them using .join() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f2939a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The children Pierre Laura and Ashley went to the store\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "text=\"The children - Pierre, Laura, and Ashley - went to the store.\"\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "lst=tokenizer.tokenize(text)\n",
    "print(' '.join(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe0970",
   "metadata": {},
   "source": [
    "### 7. Tokenization Dataframe Columns using NLTK\n",
    "Quite often you will need to tokenized data in a column of pandas dataframe. This can be achieved easily by using apply and lambda function of Python with the NLTK tokenization functions.\n",
    "\n",
    "Let us understand this better with the help of an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e43b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrases</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The greatest glory in living lies not in never...</td>\n",
       "      <td>[The, greatest, glory, in, living, lies, not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The way to get started is to quit talking and ...</td>\n",
       "      <td>[The, way, to, get, started, is, to, quit, tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If life were predictable it would cease to be ...</td>\n",
       "      <td>[If, life, were, predictable, it, would, cease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you set your goals ridiculously high and it...</td>\n",
       "      <td>[If, you, set, your, goals, ridiculously, high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Phrases  \\\n",
       "0  The greatest glory in living lies not in never...   \n",
       "1  The way to get started is to quit talking and ...   \n",
       "2  If life were predictable it would cease to be ...   \n",
       "3  If you set your goals ridiculously high and it...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [The, greatest, glory, in, living, lies, not, ...  \n",
       "1  [The, way, to, get, started, is, to, quit, tal...  \n",
       "2  [If, life, were, predictable, it, would, cease...  \n",
       "3  [If, you, set, your, goals, ridiculously, high...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example;\n",
    "import pandas as pd\n",
    "from nltk.tokenize import  word_tokenize\n",
    "\n",
    "df = pd.DataFrame({'Phrases': ['The greatest glory in living lies not in never falling, but in rising every time we fall.', \n",
    "                              'The way to get started is to quit talking and begin doing.', \n",
    "                              'If life were predictable it would cease to be life, and be without flavor.',\n",
    "                              \"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"]})\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['Phrases']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a546bd9",
   "metadata": {},
   "source": [
    "### NLTK Tokenize vs Split\n",
    "The split function is usually used to separate strings with a specified delimiter, e.g. in a tab-separated file, we can use str.split(‘\\t’) or when we are trying to split a string by the newline \\n when our textfile has one sentence per line or when we are trying to split by any specific character. But the same can’t be performed using the NLTK tokenize functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b1b0614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first line of text.\n",
      "This is the second line of text.\n",
      "['This is the first line of text.', 'This is the second line of text.']\n",
      "['This is the first line of text.\\nThis is the second line of text.']\n",
      "['Thi', ' i', ' the fir', 't line of text.\\nThi', ' i', ' the ', 'econd line of text.']\n",
      "['This', 'is', 'the', 'first', 'line', 'of', 'text.', 'This', 'is', 'the', 'second', 'line', 'of', 'text.']\n",
      "['This', 'is', 'the', 'first', 'line', 'of', 'text', '.', 'This', 'is', 'the', 'second', 'line', 'of', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"This is the first line of text.\\nThis is the second line of text.\"\n",
    "print(text)\n",
    "print(text.split('\\n')) #Spliting the text by '\\n'.\n",
    "print(text.split('\\t')) #Spliting the text by '\\t'.\n",
    "print(text.split('s'))  #Spliting by charecter 's'.\n",
    "print(text.split())  #Spliting the text by space.\n",
    "print(word_tokenize(text))    #Tokenizing by using word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acb75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
